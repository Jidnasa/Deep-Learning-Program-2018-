{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import your tensorflow as tf \n",
    "...\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading MNIST Dataset into \"MNIST_data\" and one_hot =TRUE Indicates output is of form [0 0 0 0 0 1 0 0 0 0] for 5\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your x and y_ are just going to place placeholders that basically just indicate the type of input you want in your CNN and the type of output. For each of these placeholders, you have to specify the type and the shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "sess = tf.InteractiveSession()\n",
    "# Example\n",
    "'''\n",
    "tf.placeholder(\n",
    "    dtype,\n",
    "    shape=None,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "'''\n",
    "\n",
    "# Create placeholder for x (input) of dtype float and shape [None,28,28,1]\n",
    " ...                                                   #shape in CNNs is always None x height x width x color channels\n",
    "\n",
    "    \n",
    "#Create placeholder for y (output) of dtype float and shape [None, 10]\n",
    " ...                                                    #shape is always None x number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our placeholders, we just have to specify the network architecture. Basically, the main point we have to remember is that all of the filters (weights) and biases are tensorflow variables. Let's create our filter and bias for the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Variable W_conv1 which is randomly initialised via truncated normal\n",
    "# shape is filter x filter x input channels x output channels\n",
    "\n",
    "\n",
    "#W_conv1 = tf.Variable(tf.truncated_normal([filter, filter, input, output], stddev=0.1))\n",
    "...\n",
    "\n",
    "# Create a Variable b_conv1\n",
    "b_conv1 = tf.Variable(tf.constant(.1, shape = [32])) #shape of the bias just has to match output channels of the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our filter and our bias, we can call our first conv layer. The 4 arguments you have to specify are the input (which is where our placeholder comes into play), the filter (we just created the variable for that), the stride, and the padding)\n",
    "\n",
    "### tf.nn.conv2d(input, filter, strides, padding, name=None )\n",
    "#### Input: A Tensor. Must be one of the following types: half, bfloat16, float32. A 4-D tensor. \n",
    "#### filter: A Tensor. Must have the same type as input. A 4-D tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "#### strides: A list of ints. 1-D tensor of length 4. The stride of the sliding window for each dimension of input. The dimension order is determined by the value of data_format, see below for details.\n",
    "#### padding: A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (x)\n",
    "print (W_conv1)\n",
    "\n",
    "#Create conv2d layer with input = x, filter = W_conv1, strides = [1,1,1,1] padding = \"SAME\"\n",
    "\n",
    "h_conv1 = ... + b_conv1\n",
    "\n",
    "#Apply relu activation function\n",
    "h_conv1 = tf.nn.relu(...)\n",
    "\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just defines some methods to make the function calls a little nicer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(input=x, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's just complete the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Conv and Pool Layers\n",
    "#W_conv2 = tf.Variable(tf.truncated_normal([filter, filter, input, output], stddev=0.1)) \n",
    "W_conv2 = ...\n",
    "\n",
    "b_conv2 = ...\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(... , ... ) + b_conv2)\n",
    "\n",
    "h_pool2 = max_pool_2x2(...)\n",
    "\n",
    "#First Fully Connected Layer\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(.1, shape = [1024]))\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#Dropout Layer\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#Second Fully Connected Layer\n",
    "# Create tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))\n",
    "\n",
    "W_fc2 = ...\n",
    "\n",
    "b_fc2 = tf.Variable(tf.constant(.1, shape = [10]))\n",
    "\n",
    "#Final Layer\n",
    "y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's formulate our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x = tf.constant([[1., 1.], [2., 2.]])\n",
    "tf.reduce_mean(x)  # 1.5\n",
    "tf.reduce_mean(x, 0)  # [1.5, 1.5]\n",
    "tf.reduce_mean(x, 1)  # [1.,  2.]\n",
    "\n",
    "Cross entropy\n",
    "H(y)=∑ y' * log(1/y)= − ∑y' * logy\n",
    "\n",
    "'''\n",
    "# tf.reduce_mean()\n",
    "#tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y)\n",
    "\n",
    "crossEntropyLoss = ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to use an optimizer to minimize the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using tf.train and optimizer used is AdamOptimizer to minimize crossEntropyLoss\n",
    "\n",
    "trainStep = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the statements that help with calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.argmax(input, axis=None, name=None, dimension=None)\n",
    "# Returns the index with the largest value across axis of a tensor\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line is the main statement that gets initializes all the variables we've declared earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code to allow us to visualize our training with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('Cross Enropy Loss', crossEntropyLoss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = mnist.train.next_batch(1)\n",
    "print (b[0].shape) #b[0] contains the image\n",
    "image = tf.reshape(b[0], [-1,28,28,1])\n",
    "print (image)\n",
    "my_img = image.eval() #here is your image Tensor\n",
    "my_i = my_img.squeeze()\n",
    "plt.imshow(my_i, cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 50\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(batchSize)\n",
    "    trainingInputs = batch[0].reshape([batchSize,28,28,1])\n",
    "    trainingLabels = batch[1]\n",
    "    if i%10 == 0:\n",
    "        summary = sess.run(merged, {x: trainingInputs, y_: trainingLabels, keep_prob: 1.0})\n",
    "        writer.add_summary(summary, i)\n",
    "    if i%100 == 0:\n",
    "        trainAccuracy = accuracy.eval(session=sess, feed_dict={x:trainingInputs, y_: trainingLabels, keep_prob: 1.0})\n",
    "        print (\"step %d, training accuracy %g\"%(i, trainAccuracy))\n",
    "    trainStep.run(session=sess, feed_dict={x: trainingInputs, y_: trainingLabels, keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testInputs = mnist.test.images.reshape([-1, 28, 28, 1])\n",
    "testLabels = mnist.test.labels\n",
    "acc = accuracy.eval(feed_dict = {x: testInputs, y_: testLabels, keep_prob: 1.0})\n",
    "print(\"testing accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
